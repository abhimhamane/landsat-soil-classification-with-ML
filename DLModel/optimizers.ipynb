{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01,\n",
    "                 final_lr: float = 0,\n",
    "                 decay_type: str = None) -> None:\n",
    "        self.lr = lr\n",
    "        self.final_lr = final_lr\n",
    "        self.decay_type = decay_type\n",
    "        self.first = True\n",
    "\n",
    "    def _setup_decay(self) -> None:\n",
    "\n",
    "        if not self.decay_type:\n",
    "            return\n",
    "        elif self.decay_type == 'exponential':\n",
    "            self.decay_per_epoch = np.power(self.final_lr / self.lr,\n",
    "                                       1.0 / (self.max_epochs - 1))\n",
    "        elif self.decay_type == 'linear':\n",
    "            self.decay_per_epoch = (self.lr - self.final_lr) / (self.max_epochs - 1)\n",
    "\n",
    "    def _decay_lr(self) -> None:\n",
    "\n",
    "        if not self.decay_type:\n",
    "            return\n",
    "\n",
    "        if self.decay_type == 'exponential':\n",
    "            self.lr *= self.decay_per_epoch\n",
    "\n",
    "        elif self.decay_type == 'linear':\n",
    "            self.lr -= self.decay_per_epoch\n",
    "\n",
    "    def step(self,\n",
    "             epoch: int = 0) -> None:\n",
    "\n",
    "        for (param, param_grad) in zip(self.net.params(),\n",
    "                                       self.net.param_grads()):\n",
    "            self._update_rule(param=param,\n",
    "                              grad=param_grad)\n",
    "\n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    '''\n",
    "    Stochasitc gradient descent optimizer.\n",
    "    '''    \n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01) -> None:\n",
    "        '''Pass'''\n",
    "        super().__init__(lr)\n",
    "\n",
    "    def step(self):\n",
    "        '''\n",
    "        For each parameter, adjust in the appropriate direction, with \n",
    "        the magnitude of the adjustment based on the learning rate.\n",
    "        '''\n",
    "        for (param, param_grad) in zip(self.net.params(),\n",
    "                                       self.net.param_grads()):\n",
    "\n",
    "            param -= self.lr * param_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum(Optimizer):\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01,\n",
    "                 final_lr: float = 0,\n",
    "                 decay_type: str = None,\n",
    "                 momentum: float = 0.9) -> None:\n",
    "        super().__init__(lr, final_lr, decay_type)\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def step(self) -> None:\n",
    "        if self.first:\n",
    "            self.velocities = [np.zeros_like(param)\n",
    "                               for param in self.net.params()]\n",
    "            self.first = False\n",
    "\n",
    "        for (param, param_grad, velocity) in zip(self.net.params(),\n",
    "                                                 self.net.param_grads(),\n",
    "                                                 self.velocities):\n",
    "            self._update_rule(param=param,\n",
    "                              grad=param_grad,\n",
    "                              velocity=velocity)\n",
    "\n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "\n",
    "            # Update velocity\n",
    "            kwargs['velocity'] *= self.momentum\n",
    "            kwargs['velocity'] += self.lr * kwargs['grad']\n",
    "\n",
    "            # Use this to update parameters\n",
    "            kwargs['param'] -= kwargs['velocity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad(Optimizer):\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01,\n",
    "                 final_lr_exp: float = 0,\n",
    "                 final_lr_linear: float = 0) -> None:\n",
    "        super().__init__(lr, final_lr_exp, final_lr_linear)\n",
    "        self.eps = 1e-7\n",
    "\n",
    "    def step(self) -> None:\n",
    "        if self.first:\n",
    "            self.sum_squares = [np.zeros_like(param)\n",
    "                                for param in self.net.params()]\n",
    "            self.first = False\n",
    "\n",
    "        for (param, param_grad, sum_square) in zip(self.net.params(),\n",
    "                                                   self.net.param_grads(),\n",
    "                                                   self.sum_squares):\n",
    "            self._update_rule(param=param,\n",
    "                              grad=param_grad,\n",
    "                              sum_square=sum_square)\n",
    "\n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "\n",
    "            # Update running sum of squares\n",
    "            kwargs['sum_square'] += (self.eps +\n",
    "                                     np.power(kwargs['grad'], 2))\n",
    "\n",
    "            # Scale learning rate by running sum of squareds=5\n",
    "            lr = np.divide(self.lr, np.sqrt(kwargs['sum_square']))\n",
    "\n",
    "            # Use this to update parameters\n",
    "            kwargs['param'] -= lr * kwargs['grad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedSGD(Optimizer):\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01,\n",
    "                 alpha: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def step(self) -> None:\n",
    "\n",
    "        for (param, param_grad) in zip(self.net.params(),\n",
    "                                       self.net.param_grads()):\n",
    "\n",
    "            self._update_rule(param=param,\n",
    "                              grad=param_grad)\n",
    "\n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "\n",
    "            # Use this to update parameters\n",
    "            kwargs['param'] -= (\n",
    "                self.lr * kwargs['grad'] + self.alpha * kwargs['param'])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
